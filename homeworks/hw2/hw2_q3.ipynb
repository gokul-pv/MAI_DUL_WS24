{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEcSNKhrotPo"
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "## General Tips\n",
    "In each homework problem, you will implement various autoencoder models and run them on two datasets (dataset 1 and dataset 2). The expected outputs for dataset 1 are already provided to help as a sanity check.\n",
    "\n",
    "Feel free to print whatever output (e.g. debugging code, training code, etc) you want, as the graded submission will be the submitted pdf with images.\n",
    "\n",
    "After you complete the assignment, download all of the image outputted in the results/ folder and upload them to the figure folder in the given latex template.\n",
    "\n",
    "Run the cells below to download and load up the starter code. It may take longer to run since we are using larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gokul/Study/Unsupervised-DL/Git/MAI_DUL_WS24'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get to the parent dir of mai_dul repo\n",
    "import os\n",
    "os.chdir('../../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install latest version deepul package\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepul.hw2_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-EZD8GCdx0B"
   },
   "source": [
    "# Question 3: VQ-VAE [40pts]\n",
    "In this question, you with train a [VQ-VAE](https://arxiv.org/abs/1711.00937) on the SVHN and CIFAR10. If you are confused on how the VQ-VAE works, you may find [Lilian Weng's blogpost](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#vq-vae-and-vq-vae-2) to be useful.\n",
    "\n",
    "You may experiment with different hyperparameters and architecture designs, but the following designs for the VQ-VAE architecture may be useful.\n",
    "\n",
    "```\n",
    "conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "transpose_conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "linear(in_dim, out_dim)\n",
    "batch_norm2d(dim)\n",
    "\n",
    "residual_block(dim)\n",
    "    batch_norm2d(dim)\n",
    "    relu()\n",
    "    conv2d(dim, dim, 3, 1, 1)\n",
    "    batch_norm2d(dim)\n",
    "    relu()\n",
    "    conv2d(dim, dim, 1, 1, 0)\n",
    "\n",
    "Encoder\n",
    "    conv2d(3, 256, 4, 2, 1) 16 x 16\n",
    "    batch_norm2d(256)\n",
    "    relu()\n",
    "    conv2d(256, 256, 4, 2, 1) 8 x 8\n",
    "    residual_block(256)\n",
    "    residual_block(256)\n",
    "\n",
    "Decoder\n",
    "    residual_block(256)\n",
    "    residual_block(256)\n",
    "    batch_norm2d(256)\n",
    "    relu()\n",
    "    transpose_conv2d(256, 256, 4, 2, 1) 16 x 16\n",
    "    batch_norm2d(256)\n",
    "    relu()\n",
    "    transpose_conv2d(256, 3, 4, 2, 1) 32 x 32\n",
    "```\n",
    "\n",
    "A few other tips:\n",
    "*   Use a codebook with $K = 128$ latents each with a $D = 256$ dimensional embedding vector\n",
    "*   You should initialize each element in your $K\\times D$ codebook to be uniformly random in $[-1/K, 1/K]$\n",
    "*   Use batch size 128 with a learning rate of $10^{-3}$ and an Adam optimizer\n",
    "*   Center and scale your images to $[-1, 1]$\n",
    "*   Supposing that $z_e(x)$ is the encoder output, and $z_q(x)$ is the quantized output using the codebook, you can implement the straight-through estimator as follows (where below is fed into the decoder):\n",
    "  * `(z_q(x) - z_e(x)).detach() + z_e(x)` in Pytorch\n",
    "  * `tf.stop_gradient(z_q(x) - z_e(x)) + z_e(x)` in Tensorflow.\n",
    "\n",
    "In addition to training the VQ-VAE, you will also need to train a Transformer prior on the categorical latents in order to sample. Feel free to use your implementation for HW1! You should flatten the VQ-VAE tokens into a [H x W] sequence, and use a start token.\n",
    "\n",
    "**You will provide the following deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average loss of the training data (per minibatch) and test data (for your entire test set) **for both your VQ-VAE and Transformer prior**. Code is provided that automatically plots the training curves.\n",
    "2. Report the final test set performances of your final models\n",
    "3. 100 samples from your trained VQ-VAE and Transformer prior\n",
    "4. 50 real-image / reconstruction pairs (for some $x$, encode and then decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHZsMrEw5wLN"
   },
   "source": [
    "## Solution\n",
    "Fill out the function below and return the neccessary arguments. Feel free to create more cells if need be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from homeworks.hw2.vae import VQVAE, TransformerPrior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vqvae(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x * 2 - 1\n",
    "            x = x.to(device)\n",
    "            x_recon, commitment_loss, codebook_loss, _ = model(x)\n",
    "            loss = F.mse_loss(x_recon, x) + 0.25 * commitment_loss + 0.25 * codebook_loss\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqvae(model, train_loader, test_loader, device, num_epochs=50, learning_rate=1e-3):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "    test_losses = [evaluate_vqvae(model, test_loader, device)]  # Initial test loss\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, unit='batch')\n",
    "\n",
    "        for batch_idx, (x, _) in enumerate(pbar):\n",
    "            x = x * 2 - 1  # Scale to [-1, 1]\n",
    "            x = x.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x_recon, commitment_loss, codebook_loss, _ = model(x)\n",
    "            \n",
    "            recon_loss = F.mse_loss(x_recon, x)\n",
    "            loss = recon_loss + 0.25 * commitment_loss + 0.25 * codebook_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            pbar.set_description(desc=f\"batch_loss={loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss = evaluate_vqvae(model, test_loader, device)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    return np.array(train_losses), np.array(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformer(vqvae, transformer, loader, device):\n",
    "    transformer.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            indices = vqvae.encode(x)\n",
    "            logits = transformer(indices)\n",
    "            \n",
    "            targets = indices.view(indices.shape[0], -1)\n",
    "            targets = torch.cat([targets[:, 1:], torch.full((targets.shape[0], 1), -100, device=device)], dim=1)\n",
    "            \n",
    "            loss = criterion(logits[:, :-1].reshape(-1, logits.shape[-1]), targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(vqvae, transformer, train_loader, test_loader, device, num_epochs=50, learning_rate=1e-3):\n",
    "    optimizer = optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = [evaluate_transformer(vqvae, transformer, test_loader, device)]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        transformer.train()\n",
    "        pbar = tqdm(train_loader, unit='batch')\n",
    "        \n",
    "        for batch_idx, (x, _) in enumerate(pbar):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # Get VQ-VAE encodings\n",
    "            with torch.no_grad():\n",
    "                indices = vqvae.encode(x)\n",
    "            \n",
    "            # Train transformer\n",
    "            optimizer.zero_grad()\n",
    "            logits = transformer(indices)\n",
    "            \n",
    "            # Shift predictions one step\n",
    "            targets = indices.view(indices.shape[0], -1)\n",
    "            targets = torch.cat([targets[:, 1:], torch.full((targets.shape[0], 1), -100, device=device)], dim=1)\n",
    "            \n",
    "            loss = criterion(logits[:, :-1].reshape(-1, logits.shape[-1]), targets.reshape(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            pbar.set_description(desc=f\"batch_loss={loss.item():.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss = evaluate_transformer(vqvae, transformer, test_loader, device)\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    return np.array(train_losses), np.array(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_images(vqvae, transformer, device, n_samples=100):\n",
    "    vqvae.eval()\n",
    "    transformer.eval()\n",
    "    \n",
    "    samples = []\n",
    "    for _ in range(n_samples):\n",
    "        # Start with start token\n",
    "        curr_sequence = torch.full((1, 1), 128, device=device)\n",
    "        \n",
    "        # Generate sequence\n",
    "        for _ in range(64):  # 8x8 latents\n",
    "            logits = transformer(curr_sequence)\n",
    "            probs = F.softmax(logits[:, -1], dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            curr_sequence = torch.cat([curr_sequence, next_token], dim=1)\n",
    "        \n",
    "        # Remove start token and reshape\n",
    "        indices = curr_sequence[:, 1:].view(1, 8, 8)\n",
    "        \n",
    "        # Decode\n",
    "        sample = vqvae.decode(indices)\n",
    "        sample = (sample + 1) / 2  # Scale to [0, 1]\n",
    "        sample = sample.clamp(0, 1)\n",
    "        sample = (sample * 255).cpu().numpy().transpose(0, 2, 3, 1).astype(np.uint8)\n",
    "        samples.append(sample[0])\n",
    "    \n",
    "    return np.stack(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_reconstructions(vqvae, test_loader, device, n_samples=50):\n",
    "    vqvae.eval()\n",
    "    pairs = []\n",
    "    \n",
    "    for x, _ in test_loader:\n",
    "        if len(pairs) >= n_samples:\n",
    "            break\n",
    "            \n",
    "        x = x.to(device)\n",
    "        x_recon = vqvae(x * 2 - 1)[0]\n",
    "        x_recon = (x_recon + 1) / 2\n",
    "        x_recon = x_recon.clamp(0, 1)\n",
    "        \n",
    "        for i in range(x.shape[0]):\n",
    "            if len(pairs) >= n_samples:\n",
    "                break\n",
    "                \n",
    "            orig = (x[i] * 255).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "            recon = (x_recon[i] * 255).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "            pairs.extend([orig, recon])\n",
    "    \n",
    "    return np.stack(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cUQ2V2hLdyUF"
   },
   "outputs": [],
   "source": [
    "def q3(train_data, test_data, dset_id):\n",
    "    \"\"\"\n",
    "    train_data: torch dataset with (n_train, 3, 32, 32) color images as tensors with 256 values rescaled to [0, 1]\n",
    "    test_data: torch dataset with (n_test, 3, 32, 32) color images as tensors with 256 values rescaled to [0, 1]\n",
    "    dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "               used to set different hyperparameters for different datasets\n",
    "\n",
    "    Returns\n",
    "    - a (# of training iterations,) numpy array of VQ-VAE train losess evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of VQ-VAE test losses evaluated once at initialization and after each epoch\n",
    "    - a (# of training iterations,) numpy array of Transformer prior train losess evaluated every minibatch\n",
    "    - a (# of epochs + 1,) numpy array of Transformer prior test losses evaluated once at initialization and after each epoch\n",
    "    - a (100, 32, 32, 3) numpy array of 100 samples with values in {0, ... 255}\n",
    "    - a (100, 32, 32, 3) numpy array of 50 real image / reconstruction pairs\n",
    "      FROM THE TEST SET with values in [0, 255]\n",
    "    \"\"\"\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                         else \"mps\" if torch.backends.mps.is_available() \n",
    "                         else \"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # DataLoader settings based on device\n",
    "    kwargs = {'num_workers': 8, 'pin_memory': False} if torch.cuda.is_available() else \\\n",
    "             {'num_workers': 0} if torch.backends.mps.is_available() else \\\n",
    "             {}\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 128\n",
    "    learning_rate = 1e-3\n",
    "    num_epochs = 1\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    # Initialize models\n",
    "    vqvae = VQVAE().to(device)\n",
    "    transformer = TransformerPrior().to(device)\n",
    "    \n",
    "    # Train VQ-VAE\n",
    "    vqvae_train_losses, vqvae_test_losses = train_vqvae(\n",
    "        vqvae, train_loader, test_loader, device, num_epochs, learning_rate\n",
    "    )\n",
    "    \n",
    "    # Train Transformer\n",
    "    transformer_train_losses, transformer_test_losses = train_transformer(\n",
    "        vqvae, transformer, train_loader, test_loader, device, num_epochs, learning_rate\n",
    "    )\n",
    "    \n",
    "    # Generate samples\n",
    "    samples = sample_images(vqvae, transformer, device, n_samples=100)\n",
    "    \n",
    "    # Get reconstructions\n",
    "    reconstructions = get_reconstructions(vqvae, test_loader, device, n_samples=50)\n",
    "    \n",
    "    return (\n",
    "        vqvae_train_losses,\n",
    "        vqvae_test_losses,\n",
    "        transformer_train_losses,\n",
    "        transformer_test_losses,\n",
    "        samples,\n",
    "        reconstructions\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nbn-r53G51X_"
   },
   "source": [
    "## Results\n",
    "Once you've finished `q3`, execute the cells below to visualize and save your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ClKjwiAd535z",
    "outputId": "dc4baa34-e868-4e32-f35d-937f3aab2b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: homeworks/hw2/data/train_32x32.mat\n",
      "Using downloaded and verified file: homeworks/hw2/data/test_32x32.mat\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ann/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c51dc4678d418e985aba12129e049f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/573 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (65) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mq3_save_results\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq3\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Study/Unsupervised-DL/Git/MAI_DUL_WS24/deepul/hw2_helper.py:153\u001b[0m, in \u001b[0;36mq3_save_results\u001b[0;34m(dset_id, fn)\u001b[0m\n\u001b[1;32m    150\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m CIFAR10(root\u001b[38;5;241m=\u001b[39mdata_dir, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mToTensor(), download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    151\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m CIFAR10(root\u001b[38;5;241m=\u001b[39mdata_dir, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mToTensor(), download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 153\u001b[0m vqvae_train_losses, vqvae_test_losses, prior_train_losses, prior_test_losses, samples, reconstructions \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdset_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m samples, reconstructions \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m), reconstructions\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVQ-VAE Final Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvqvae_test_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m, in \u001b[0;36mq3\u001b[0;34m(train_data, test_data, dset_id)\u001b[0m\n\u001b[1;32m     45\u001b[0m vqvae_train_losses, vqvae_test_losses \u001b[38;5;241m=\u001b[39m train_vqvae(\n\u001b[1;32m     46\u001b[0m     vqvae, train_loader, test_loader, device, num_epochs, learning_rate\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Train Transformer\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m transformer_train_losses, transformer_test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvqvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Generate samples\u001b[39;00m\n\u001b[1;32m     55\u001b[0m samples \u001b[38;5;241m=\u001b[39m sample_images(vqvae, transformer, device, n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mtrain_transformer\u001b[0;34m(vqvae, transformer, train_loader, test_loader, device, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      5\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m [\u001b[43mevaluate_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvqvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      9\u001b[0m     transformer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mevaluate_transformer\u001b[0;34m(vqvae, transformer, loader, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m indices \u001b[38;5;241m=\u001b[39m vqvae\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m---> 10\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m targets \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mview(indices\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([targets[:, \u001b[38;5;241m1\u001b[39m:], torch\u001b[38;5;241m.\u001b[39mfull((targets\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ann/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ann/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Study/Unsupervised-DL/Git/MAI_DUL_WS24/homeworks/hw2/vae/model_vqvae.py:133\u001b[0m, in \u001b[0;36mTransformerPrior.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Embed tokens and add positional encoding\u001b[39;00m\n\u001b[1;32m    132\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m--> 133\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embedding\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Run through transformer\u001b[39;00m\n\u001b[1;32m    136\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (65) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "q3_save_results(1, q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vDEvml-59zA"
   },
   "outputs": [],
   "source": [
    "q3_save_results(2, q3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "16l6wsRW4k8d",
    "pOS0PKRKdtLS"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
